{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Установим все зависимости","metadata":{}},{"cell_type":"code","source":"! pip install av\n\n! pip install openai\n!pip install torchaudio omegaconf #Silero\n! pip install git+https://github.com/m-bain/whisperx.git\n! pip install git+https://github.com/openai/whisper.git\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T15:11:15.441727Z","iopub.execute_input":"2023-03-29T15:11:15.442687Z","iopub.status.idle":"2023-03-29T15:11:15.447572Z","shell.execute_reply.started":"2023-03-29T15:11:15.442634Z","shell.execute_reply":"2023-03-29T15:11:15.446138Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nimport av\nimport openai\nimport numpy as np\nfrom pathlib import Path\nfrom scipy.io import wavfile\nfrom scipy import fft\nimport re\nimport json\nimport statistics\nfrom Levenshtein import distance as levenshtein_distance\nimport torch\nfrom pprint import pprint\nfrom omegaconf import OmegaConf\nfrom IPython.display import Audio, display\nimport requests\nimport json\nimport random\n\napi_key_openai = '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'\napi_key_yandex = 'AQVNyNsmYewynukDriF-XtGV-GCTNZoOUQswy8bb'\napi_key_yandex_tr = 'AQVNzZ0MZxU2R4AB4857U4HdU_9mkp_KHgNdTh2H'\n\nfolder_id = 'b1ggo7gjgm7p6d3vd83t'\nopenai.api_key = api_key_openai","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:12:30.661257Z","iopub.execute_input":"2023-03-29T09:12:30.661807Z","iopub.status.idle":"2023-03-29T09:12:30.760066Z","shell.execute_reply.started":"2023-03-29T09:12:30.661757Z","shell.execute_reply":"2023-03-29T09:12:30.757794Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# для теста, обрежем видео\n# import os\n\n# def trim_video(input_file, output_file, start_time, duration):\n#     cmd = f\"ffmpeg -i {input_file} -ss {start_time} -t {duration} -c copy -map 0 {output_file}\"\n#     os.system(cmd)\n    \n\n# output_file = \"Movie.mkv\"\n# start_time = \"00:00:00\"  # Начало обрезки видео (формат: HH:MM:SS)\n# duration = \"00:29:30\"    # Продолжительность обрезанного видео (формат: HH:MM:SS)\n# input_video = '/kaggle/input/audiodescription-nemo/Movie.mkv'\n# trim_video(input_video, output_file, start_time, duration)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T15:11:02.934613Z","iopub.execute_input":"2023-03-29T15:11:02.935154Z","iopub.status.idle":"2023-03-29T15:11:02.962062Z","shell.execute_reply.started":"2023-03-29T15:11:02.935110Z","shell.execute_reply":"2023-03-29T15:11:02.960989Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"movie_path = 'Movie.mkv'\nen_tiflo_orig = '/kaggle/input/audiodescription-nemo/en_tiflo.mp3'\nmovie_title = \"В поисках немо\"\nwhisper_flag = True\nsilero = False\ngpt_translate = False\n\n\nen_tiflo = 'en_tiflo.wav'\nru_without = 'ru_without.wav'\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:41:13.055160Z","iopub.execute_input":"2023-03-29T12:41:13.055895Z","iopub.status.idle":"2023-03-29T12:41:13.064775Z","shell.execute_reply.started":"2023-03-29T12:41:13.055825Z","shell.execute_reply":"2023-03-29T12:41:13.062842Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# Извлечение аудиодорожек из фильма","metadata":{}},{"cell_type":"code","source":"def extract_audio_tracks(input_video: str) -> None:\n    \"\"\"\n    Extracts English and Russian audio tracks from a video file and saves them to separate audio files.\n\n    Args:\n        input_video (str): Path to the input video file.\n        english_audio_track (str): Path to the output file for the English audio track.\n        russian_audio_track (str): Path to the output file for the Russian audio track.\n\n    Raises:\n        RuntimeError: If the English or Russian audio tracks cannot be found in the video file.\n    \"\"\"\n    english_audio_track = \"en_without.wav\" \n    russian_audio_track = \"ru_without.wav\"\n    # Получение индексов аудиодорожек ru и en\n    container = av.open(input_video)\n\n    eng_track_index = None\n    rus_track_index = None\n\n    for stream in container.streams.audio:\n        lang = stream.metadata.get(\"language\")\n        if lang == \"eng\":\n            eng_track_index = stream.index\n        elif lang == \"rus\":\n            rus_track_index = stream.index\n\n    if eng_track_index is None or rus_track_index is None:\n        raise RuntimeError(\"Не удалось найти английскую или русскую аудиодорожку.\")\n\n    # Извлечение аудиодорожек\n    os.system(f\"ffmpeg -i {input_video} -map 0:{eng_track_index} -c:a pcm_s16le {english_audio_track}\")\n    os.system(f\"ffmpeg -i {input_video} -map 0:{rus_track_index} -c:a pcm_s16le {russian_audio_track}\")\n\n    print(\"Аудиодорожки извлечены.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:39.934671Z","iopub.execute_input":"2023-03-29T09:14:39.935188Z","iopub.status.idle":"2023-03-29T09:14:39.947899Z","shell.execute_reply.started":"2023-03-29T09:14:39.935139Z","shell.execute_reply":"2023-03-29T09:14:39.946048Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Приведение в нужный формат","metadata":{}},{"cell_type":"code","source":"def get_audio_info(audio_file):\n    ffprobe_command = f'ffprobe -v error -select_streams a:0 -show_entries stream=channels,sample_rate,duration -of default=noprint_wrappers=1:nokey=1 \"{audio_file}\"'\n    output = subprocess.check_output(ffprobe_command + ' -v quiet', shell=True, text=True).strip().split(\"\\n\")\n    sample_rate, channels, input_duration = int(output[0]), int(output[1]), float(output[2])\n    return channels, sample_rate, input_duration\n\ndef convert_mp3_to_wav(input_file, output_file):\n    \"\"\"\n    Конвертирует MP3-файл в WAV-файл с помощью FFmpeg.\n\n    Параметры:\n    input_file (str): Путь к исходному MP3-файлу.\n    output_file (str): Путь для сохранения WAV-файла.\n    \"\"\"\n\n    # Команда для преобразования MP3-файла в WAV-файл\n    command = f\"ffmpeg -i {input_file} {output_file}\"\n\n    # Выполнение команды с помощью os.system()\n    os.system(command)\n\ndef polychanels2stereo(input_file):\n    output_file = \"audio_stereo.wav\"\n    # Преобразование 6-канального аудио в стерео\n    os.system(f\"ffmpeg -i {input_file} -ac 2 {output_file}\")\n    open(input_file, 'w').close() # только для колаба\n    os.remove(input_file)\n    os.rename(output_file, input_file)\n\n\ndef audio2mono(input_file):\n    output_file = \"output_mono.wav\"\n    cmd_command = f'ffmpeg -i {input_file} -ac 1 {output_file}'\n    os.system(cmd_command)\n    open(input_file, 'w').close() # только для колаба, он при удалении сохраняет файл в корзину..\n    os.remove(input_file)\n    os.rename(output_file, input_file)\n    \ndef channels_changer(ru_poly_to_stereo=False):\n    if ru_poly_to_stereo:\n        ru_channels,_,_ = get_audio_info('ru_without.wav')\n        # преобразует 5d и т.д. звук в стерео\n        if ru_channels > 2:\n            polychanels2stereo('ru_without.wav')\n\n    en_tiflo_channels,_,_ = get_audio_info('en_tiflo.wav')\n    if en_tiflo_channels > 1:\n        audio2mono('en_tiflo.wav')\n    en_without_channels,_,_ = get_audio_info('en_without.wav')\n    if en_tiflo_channels > 1:\n        audio2mono('en_without.wav')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:43.411561Z","iopub.execute_input":"2023-03-29T09:14:43.412057Z","iopub.status.idle":"2023-03-29T09:14:43.428239Z","shell.execute_reply.started":"2023-03-29T09:14:43.412015Z","shell.execute_reply":"2023-03-29T09:14:43.426908Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Синхронизация аудиодорожек\n\nЧерез преобразование фурье синхронизируем английскую дорожку к русской \nи делаем такую же длительность.","metadata":{}},{"cell_type":"code","source":"\n\n# код переделан из repo https://github.com/rpuntaie/syncstart/blob/main/syncstart.py\n\ndef normalize_audio(audio_file, duration):\n    \"\"\"Преобразует аудиофайл в WAV формат с помощью FFmpeg, \n    нормализует и преобразует дискретизацию в 16 бит.\"\"\"\n    ffmpeg_command = f'ffmpeg -i \"{audio_file}\" -t {duration} -c:a pcm_s16le -map 0:a \"{audio_file.stem}_20s.wav\"'\n    os.system(ffmpeg_command + ' -v quiet')\n    audio_file_part = f\"{audio_file.stem}_20s.wav\"\n    rate, signal = wavfile.read(audio_file_part)\n    os.remove(audio_file_part)\n    if len(signal.shape) > 1:  # если стерео, берем только первый канал (моно)\n        signal = signal[:, 0]\n    return rate, signal\n\ndef correlate_audio_files(signal1, signal2):\n    \"\"\"Вычисляет корреляцию между двумя сигналами и \n    возвращает размеры сигналов, размер паддинга, \n    позицию максимального значения и значения корреляции.\"\"\"\n    length1 = len(signal1)\n    length2 = len(signal2)\n    pad_size = length1 + length2 + 1\n    pad_size = 2 ** (int(np.log(pad_size) / np.log(2)) + 1)\n    padded_signal1 = np.zeros(pad_size)\n    padded_signal1[:length1] = signal1\n    padded_signal2 = np.zeros(pad_size)\n    padded_signal2[:length2] = signal2\n    correlation = fft.ifft(fft.fft(padded_signal1) * np.conj(fft.fft(padded_signal2)))\n    absolute_correlation = np.absolute(correlation)\n    max_position = np.argmax(absolute_correlation)\n    return length1, length2, pad_size, max_position, absolute_correlation\n\ndef synchronize_audio_files(audio_file1, audio_file2, duration=60):\n    \"\"\"Синхронизирует два аудиофайла. Первый берется за образец.\n    Возвращает имя второго файла, который нужно обрезать или добавить задержку\n    и смещение в секундах и длину первого файла, образца.\"\"\"\n    rate1, signal1 = normalize_audio(audio_file1, duration)\n    rate2, signal2 = normalize_audio(audio_file2, duration)\n    \n    #TODO приведение к одной частоте дискретизации librosa.core.resample()\n    assert rate1 == rate2, \"Частоты дискретизации не совпадают\"\n    rate = rate1\n\n    length1, length2, pad_size, max_position, _ = correlate_audio_files(signal1, signal2)\n\n    if max_position > pad_size // 2:\n        file_to_cut, offset = audio_file2, (pad_size - max_position) / rate\n    else:\n        file_to_cut, offset = audio_file1, max_position / rate\n    \n    if file_to_cut == audio_file1:\n        cut_flag = False\n    else:\n        cut_flag = True\n    return audio_file2, offset, cut_flag\n\ndef get_audio_info(audio_file):\n    ffprobe_command = f'ffprobe -v error -select_streams a:0 -show_entries stream=channels,sample_rate,duration -of default=noprint_wrappers=1:nokey=1 \"{audio_file}\"'\n    output = subprocess.check_output(ffprobe_command + ' -v quiet', shell=True, text=True).strip().split(\"\\n\")\n    sample_rate, channels, input_duration = int(output[0]), int(output[1]), float(output[2])\n    return channels, sample_rate, input_duration\n\n\ndef process_audio(audio_file, offset, cut_flag=True, duration=None):\n    audio_file = Path(audio_file)\n    output_file_ = f\"{audio_file.stem}_preprocessed.wav\"\n\n    channels, sample_rate, input_duration  = get_audio_info(audio_file)\n    channel_layout = 'stereo' if channels == 2 else 'mono'\n\n    if cut_flag:\n\n        ffmpeg_command = f'ffmpeg -i \"{audio_file}\" -ss {offset} -t {duration} -c:a pcm_s16le -map 0:a \"{output_file_}\"'\n        os.system(ffmpeg_command + ' -v quiet')\n        curr_duration = input_duration - offset\n    else:\n\n        silence_file = \"silence.wav\"\n        ffmpeg_silence_command = f'ffmpeg -f lavfi -i anullsrc=channel_layout={channel_layout}:sample_rate={sample_rate} -t {offset} -c:a pcm_s16le \"{silence_file}\"'\n        os.system(ffmpeg_silence_command + ' -v quiet')\n        output_file_with_silence = f\"{audio_file.stem}_with_silence.wav\"\n        ffmpeg_concat_command = f'ffmpeg -i \"{silence_file}\" -i \"{audio_file}\" -filter_complex \"[0:a][1:a]concat=n=2:v=0:a=1\" -c:a pcm_s16le \"{output_file_with_silence}\"'\n        os.system(ffmpeg_concat_command + ' -v quiet')\n        os.remove(silence_file)\n        ffmpeg_command = f'ffmpeg -i \"{output_file_with_silence}\" -t {duration} -c:a pcm_s16le -map 0:a \"{output_file_}\"'\n        os.system(ffmpeg_command + ' -v quiet')\n        os.remove(output_file_with_silence)\n        curr_duration = input_duration + offset\n    \n    output_file = f\"{audio_file.stem}_processed.wav\"\n    if curr_duration < duration:\n        silence_duration = duration - curr_duration\n        print(silence_duration)\n        silence_end_file = \"silence_end.wav\"\n        ffmpeg_silence_end_command = f'ffmpeg -f lavfi -i anullsrc=channel_layout={channel_layout}:sample_rate={sample_rate} -t {silence_duration} -c:a pcm_s16le \"{silence_end_file}\"'\n        os.system(ffmpeg_silence_end_command + ' -v quiet')\n        ffmpeg_concat_end_command = f'ffmpeg -i \"{output_file_}\" -i \"{silence_end_file}\" -filter_complex \"[0:a][1:a]concat=n=2:v=0:a=1\" -c:a pcm_s16le \"{output_file}\"'\n        os.system(ffmpeg_concat_end_command + ' -v quiet')\n        os.remove(silence_end_file)\n        os.remove(output_file_)\n    else:\n        os.rename(output_file_, output_file)\n\n    return output_file\n\n\ndef synchronize_and_process_audio(audio_file1, audio_file2):\n    \"\"\"Синхронизирует два аудиофайла и записывает результат в новый файл с добавлением '_sync'\n    в название. Возвращает новый сигнал и новую частоту дискретизации.\"\"\"\n    # Синхронизируем два аудиофайла\n    file_to_cut, offset, cut_flag = synchronize_audio_files(audio_file1, audio_file2)\n\n    # Обрабатываем аудиофайл\n    sample_rate, audio_data = wavfile.read(audio_file1)\n    length_required = len(audio_data) / sample_rate\n    output = process_audio(file_to_cut, offset, cut_flag, length_required)\n    return output\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:45.565665Z","iopub.execute_input":"2023-03-29T09:14:45.566614Z","iopub.status.idle":"2023-03-29T09:14:45.596384Z","shell.execute_reply.started":"2023-03-29T09:14:45.566525Z","shell.execute_reply":"2023-03-29T09:14:45.595136Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Распознавание субтитров","metadata":{}},{"cell_type":"markdown","source":"## Yandex speechkit","metadata":{}},{"cell_type":"code","source":"# import os\n# import subprocess\n# import time \n\n# from IPython.display import Audio, display","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Whisper","metadata":{}},{"cell_type":"markdown","source":"нужно дополнительно менять окружение в kaggle. Так не работает\n","metadata":{}},{"cell_type":"code","source":"def whisperx_recognize(model='large'):\n    \"\"\"Распознавание занимает много времени\n    Примерно равное продолжительности самого аудио\"\"\"\n    # Путь к аудиофайлу, который мы будем транскрибировать\n    audio_file_path = './en_tiflo_processed.wav'\n\n\n    # Команда для запуска распознавания речи с использованием модели\n    cmd_command = f'whisperx {audio_file_path} --model {model} --output_dir . --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --align_extend 2'\n    os.system(cmd_command)\n\n    # Путь к аудиофайлу, который мы будем транскрибировать\n    audio_file_path = './en_without_processed.wav'\n\n    # Команда для запуска распознавания речи с использованием модели\n    cmd_command = f'whisperx {audio_file_path} --model {model} --output_dir . --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --align_extend 2'\n    os.system(cmd_command)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:52.765162Z","iopub.execute_input":"2023-03-29T09:14:52.766937Z","iopub.status.idle":"2023-03-29T09:14:52.774656Z","shell.execute_reply.started":"2023-03-29T09:14:52.766850Z","shell.execute_reply":"2023-03-29T09:14:52.773352Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Синхронизация субтитров по времени:","metadata":{}},{"cell_type":"markdown","source":"## Обработка .ass файлов","metadata":{}},{"cell_type":"code","source":"\n\n\ndef parse_time(time_str):\n    h, m, s = map(float, time_str.split(':'))\n    return h * 3600 + m * 60 + s\n\ndef parse_line(line):\n    line_data = line.split(',')\n    start_time = parse_time(line_data[1])\n    end_time = parse_time(line_data[2])\n    text = ','.join(line_data[9:])\n    return start_time, end_time, text\n\ndef clean_text(text):\n    text = re.sub(r'{\\\\1c&HFF00&\\\\u1}', '', text)\n    text = re.sub(r'{\\\\r}', '', text)\n    return text.strip()\n\ndef parse_ass(file):\n    with open(file, 'r', encoding='utf-8') as f:\n        content = f.readlines()\n\n    results = []\n    temp_timings = []\n    temp_words = []\n    curr_sentence = \"\"\n\n    for line in content:\n        if line.startswith(\"Dialogue:\"):\n            start_time, end_time, text = parse_line(line)\n            words = re.findall(r'{\\\\1c&HFF00&\\\\u1}(.*?){\\\\r}', text)\n            if words:\n                clean_line_text = clean_text(text)\n                if clean_line_text != curr_sentence:\n                    if temp_words:\n                        median_phrase_time = statistics.median([timing[0] for timing in temp_timings])\n                        results[-1][2] = median_phrase_time\n                        results[-1][1] = temp_timings[-1][1]\n                        results[-1][-1].extend([(timing[0], timing[1], word) for timing, word in zip(temp_timings, temp_words)])\n                        temp_timings = []\n                        temp_words = []\n                    curr_sentence = clean_line_text\n                    results.append([start_time, None, None, curr_sentence, []])\n\n                temp_timings.extend([(start_time, end_time) for _ in words])\n                temp_words.extend(words)\n\n    if temp_words:\n        results[-1][1] = temp_timings[-1][1]\n        median_phrase_time = statistics.median([timing[0] for timing in temp_timings])\n        results[-1][2] = median_phrase_time\n        results[-1][-1].extend([(timing[0], timing[1], word) for timing, word in zip(temp_timings, temp_words)])\n\n    return results\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:55.117323Z","iopub.execute_input":"2023-03-29T09:14:55.117814Z","iopub.status.idle":"2023-03-29T09:14:55.135586Z","shell.execute_reply.started":"2023-03-29T09:14:55.117710Z","shell.execute_reply":"2023-03-29T09:14:55.133735Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef levenshtein_distance_(s1, s2):\n    distance = levenshtein_distance(s1, s2)\n    length1 = len(s1)\n    length2 = len(s2)\n    length = max(length1, length2)\n    length_err = abs(length2-length1)/length\n    lev_err = distance/length\n    return lev_err, length_err\n\n\ndef word_levenshtein_distance_(s1, s2):\n    words1 = s1.split()\n    words2 = s2.split()\n\n    # Initialize matrix with zeros\n    rows = len(words1)+1\n    cols = len(words2)+1\n    distance_matrix = [[0 for _ in range(cols)] for _ in range(rows)]\n\n    # Fill matrix with Levenshtein distances\n    for i in range(1, rows):\n        for j in range(1, cols):\n            deletion = distance_matrix[i-1][j] + 1\n            insertion = distance_matrix[i][j-1] + 1\n            substitution = distance_matrix[i-1][j-1]\n            if words1[i-1] != words2[j-1]:\n                substitution += 1\n            distance_matrix[i][j] = min(deletion, insertion, substitution)\n\n    length1 = len(words1)\n    length2 = len(words2)\n    length = max(length1, length2)\n    length_err = abs(length2-length1)/length\n    # Return the Levenshtein distance\n    return (distance_matrix[rows-1][cols-1]/length + length_err)/2\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:14:56.952876Z","iopub.execute_input":"2023-03-29T09:14:56.953989Z","iopub.status.idle":"2023-03-29T09:14:56.970228Z","shell.execute_reply.started":"2023-03-29T09:14:56.953936Z","shell.execute_reply":"2023-03-29T09:14:56.968562Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Найти все реплики, которые есть во вторых, но нет в первых субтитрах и записать их в список:","metadata":{}},{"cell_type":"code","source":"\n\ndef compare_sentences(line1, line2):\n    # сравнение предложений здесь, возвращая True, если они считаются одинаковыми\n    \n    start_time1, end_time1, median_time1, sentence1, words1 = line1\n    start_time2, end_time2, median_time2, sentence2, words2 = line2\n    ev_err, length_err = levenshtein_distance_(sentence1, sentence2)\n    \n    timing_err = abs(median_time2 - median_time1) / 2 # если разница больше 1.2 секунд\n\n    for err in [ev_err, length_err, timing_err]:\n        if err > 0.6:\n            return False\n    return True\n\n\ndef intervals_intersect(start1, end1, start2, end2):\n    return not (end1 < start2 or end2 < start1)\n\n\ndef check_for_silence(subtitle1, subtitle2, index1):\n    start_time1, end_time1, median_time1, text1, words1 = subtitle1[index1]\n    min\n\n    for index2 in range(len(subtitle2)):\n        start_time2, end_time2, median_time2, text2, words2 = subtitle2[index2]\n        \n        if intervals_intersect(start_time1-0.15, end_time1+0.15, start_time2-0.15, end_time2+0.15):\n            return False\n        \n        # Выход из цикла, если start_time2 больше end_time1\n        if start_time2 > end_time1+5:\n            break\n            \n    return True\n\ndef compare_subtitles(subtitle1, subtitle2):\n    result = []\n\n    for index1 in range(len(subtitle1)):\n        start_time1, end_time1, median_time1, text1, words1 = subtitle1[index1]\n        found_match = False\n\n        for index2 in range(len(subtitle2)):\n            start_time2, end_time2, median_time2, text2, words2 = subtitle2[index2]\n\n            if abs(median_time1 - median_time2) <= 5:\n  \n                if compare_sentences(subtitle1[index1], subtitle2[index2]):\n                    found_match = True\n                    break\n            elif start_time1 < start_time2 - 5:\n                break\n\n        if not found_match:\n            if check_for_silence(subtitle1, subtitle2, index1):\n                result.append(subtitle1[index1])\n\n    return result\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:42:38.203128Z","iopub.execute_input":"2023-03-29T12:42:38.203745Z","iopub.status.idle":"2023-03-29T12:42:38.220206Z","shell.execute_reply.started":"2023-03-29T12:42:38.203697Z","shell.execute_reply":"2023-03-29T12:42:38.218329Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"### Проблемы:\nнеправильно распознаны интревалы начала, окончания предложения. \n\nВариант:\n- брать медиану времени начала от слов реплики\n- затем, вычисляем интервал, как min(median-t_start, t_end-median) median +- delta будет интервалом реплики\n\n- лезть внутрь библиотеки и вытаскивать оттуда нормальные эмбединги\n","metadata":{}},{"cell_type":"markdown","source":"### Группировка субтитров в предложения\n\nПравила:\n\n- предварительная проверка интревала субтитров, если начало и конец лежат в пределах интревала 1.2 min(median-t_start, t_end-median) оставляем как есть, иначе задаем новый интревал\n- если между репликами прошло меньше 1.5 секунды - объединяем\n","metadata":{}},{"cell_type":"code","source":"def new_time_interval(t_start, t_end, t_median):\n    sent_range = max(min(abs(t_median-t_start), abs(t_end-t_median)), 0.6)\n    if t_start < t_median - 1.2 * sent_range:\n        t_start = t_median - sent_range\n    if t_end > t_median + 1.2 * sent_range:\n        t_end = t_median + sent_range\n    return round(t_start,2), round(t_end,2), round(t_median,2)\n\ndef merge_subtitles(subtitles):\n    merged_subtitles = []\n    current_subtitle = None\n\n    for i in range(len(subtitles)):\n        start_time, end_time, median_time, text, words = subtitles[i]\n        start_time, end_time, median_time = new_time_interval(start_time, end_time, median_time)\n\n        if current_subtitle is not None:\n            if start_time - current_subtitle[2] <= 1.8:\n                current_subtitle[0] += ' ' + text\n                current_subtitle[2] = end_time\n            else:\n                merged_subtitles.append(current_subtitle)\n                current_subtitle = [text, start_time, end_time]\n        else:\n            current_subtitle = [text, start_time, end_time]\n\n    if current_subtitle is not None:\n        merged_subtitles.append(current_subtitle)\n\n    return merged_subtitles\n\n# for sentence_block in tiflo_sentence:\n#     t_start, t_end, t_median, sentence1, words1 = sentence_block\n#     t_start, t_end, t_median = new_time_interval(t_start, t_end, t_median)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:43:04.724531Z","iopub.execute_input":"2023-03-29T12:43:04.725028Z","iopub.status.idle":"2023-03-29T12:43:04.739306Z","shell.execute_reply.started":"2023-03-29T12:43:04.724983Z","shell.execute_reply":"2023-03-29T12:43:04.737867Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"# Перевод GPT API или Яндекс","metadata":{}},{"cell_type":"code","source":"\n\ndef translate_subtitles(text, movie_title):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0301\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a professional assistant that translates English text to Russian. Provide the translation without any additional comments.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Don't skip the newline characters, keep them in the text, keep the text structure. Translate the following English subtitles from the movie '{movie_title}' to Russian: '{text}'\"\n            }\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\n\n\n\ndef yandex_translate(text, target_language='ru', api_key=api_key_yandex_tr, folder_id=folder_id):\n    url = 'https://translate.api.cloud.yandex.net/translate/v2/translate'\n    headers = {'Authorization': f'Api-Key {api_key}'}\n    body = {\n        'folderId': folder_id,\n        'targetLanguageCode': target_language,\n        'texts': [text],\n    }\n    response = requests.post(url, json=body, headers=headers)\n    response_data = json.loads(response.text)\n    translations = response_data['translations']\n    translated_text = translations[0]['text']\n    return translated_text\n\n\n\n\n\ndef tokens_count(lines):\n    # Разделение каждого блока на слова по пробелу\n    words = []\n    for line in lines:\n        line_words = line.split()\n        words.extend(line_words)\n    # Подсчет количества слов и токенов\n    num_words = len(words)\n    num_tokens = num_words * 4\n    return num_tokens\n\ndef gpt_translate(movie_title, tiflo_comments):\n    max_tokens = 1800\n    gpt_text = '\\n'.join([comment[0] for comment in tiflo_comments])\n    gpt_lines_list = gpt_text.split('\\n')\n    num_tokens = tokens_count(gpt_lines_list)\n    if num_tokens > max_tokens:\n        n_blocks = -int(-num_tokens // max_tokens)\n    else:\n        n_blocks = 1\n    one_block_lines = - int(-len(gpt_lines_list) // n_blocks)\n    translated_text = []\n        \n    for i in range(n_blocks):\n        start_idx = i * one_block_lines\n        end_idx = (i + 1) * one_block_lines\n        block_lines = gpt_lines_list[start_idx:end_idx]\n        block_text = '\\n'.join(block_lines)\n        gpt_response = ''\n        n_tray = 0\n        while len(block_text.split('\\n')) != len(gpt_response.split('\\n')):\n            if gpt_translate:\n                gpt_response = translate_subtitles(block_text, movie_title)\n            else:\n                gpt_response = yandex_translate(block_text)\n            n_tray+=1\n            if n_tray > 3:\n                gpt_response = yandex_translate(block_text)\n            if n_tray > 4:\n                raise ValueError(\"Translation API failed too many times.\") \n    \n        translated_text += gpt_response.split('\\n')\n    return '\\n'.join(translated_text)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:44:08.015735Z","iopub.execute_input":"2023-03-29T12:44:08.016170Z","iopub.status.idle":"2023-03-29T12:44:08.034825Z","shell.execute_reply.started":"2023-03-29T12:44:08.016136Z","shell.execute_reply":"2023-03-29T12:44:08.033317Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"### Решить вопрос с английскими словами, привести их к русскому написанию\n! Только если используешь Silero\n\n","metadata":{}},{"cell_type":"code","source":"def eng_to_rus_translit(word):\n    eng_to_rus_dict = {\n        'a': 'а', 'b': 'б', 'c': 'к', 'd': 'д', 'e': 'е', 'f': 'ф', 'g': 'г', 'h': 'х', 'i': 'и', 'j': 'дж', 'k': 'к', 'l': 'л', 'm': 'м', 'n': 'н', 'o': 'о', 'p': 'п', 'q': 'к', 'r': 'р', 's': 'с', 't': 'т', 'u': 'у', 'v': 'в', 'w': 'в', 'x': 'кс', 'y': 'й', 'z': 'з', 'A': 'А', 'B': 'Б', 'C': 'К', 'D': 'Д', 'E': 'Е', 'F': 'Ф', 'G': 'Г', 'H': 'Х', 'I': 'И', 'J': 'Дж', 'K': 'К', 'L': 'Л', 'M': 'М', 'N': 'Н', 'O': 'О', 'P': 'П', 'Q': 'К', 'R': 'Р', 'S': 'С', 'T': 'Т', 'U': 'У', 'V': 'В', 'W': 'В', 'X': 'Кс', 'Y': 'Й', 'Z': 'З', ' ': ' '\n    }\n    \n    russian_word = ''\n    for char in word:\n        if char in eng_to_rus_dict:\n            russian_word += eng_to_rus_dict[char]\n        else:\n            russian_word += char\n\n    return russian_word\n\n\ndef replace_english_words(text):\n    english_words = re.findall(r'[A-Za-z]+', text)\n    for word in english_words:\n        text = text.replace(word, eng_to_rus_translit(word))\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:04.510336Z","iopub.execute_input":"2023-03-29T09:15:04.510789Z","iopub.status.idle":"2023-03-29T09:15:04.523427Z","shell.execute_reply.started":"2023-03-29T09:15:04.510750Z","shell.execute_reply":"2023-03-29T09:15:04.521651Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Собираем текст обратно, проставляем тайминги","metadata":{}},{"cell_type":"code","source":"def translation2subs(translated_text, tiflo_comments):\n    translated_text_list = translated_text.split('\\n')\n    translated_tiflo = []\n\n    for n, comment in enumerate(tiflo_comments):\n        translated_tiflo.append(comment.copy())\n        if silero:\n            text_line = replace_english_words(translated_text_list[n])\n        else:\n            text_line = translated_text_list[n]\n        translated_tiflo[-1][0] = text_line\n    return translated_tiflo","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:05.852823Z","iopub.execute_input":"2023-03-29T09:15:05.853252Z","iopub.status.idle":"2023-03-29T09:15:05.863614Z","shell.execute_reply.started":"2023-03-29T09:15:05.853217Z","shell.execute_reply":"2023-03-29T09:15:05.861419Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Синтез речи Яндекс","metadata":{}},{"cell_type":"code","source":"import time\ndef synthesize(text):\n    url = 'https://tts.api.cloud.yandex.net/speech/v1/tts:synthesize'\n    headers = {'Authorization': f\"Api-Key {api_key_yandex}\"}\n \n    data = {\n        'folderId': folder_id,\n        'text': text,\n        'lang': 'ru-RU',\n        'voice':'jane',\n        'emotion':'good',\n        'speed':'1.5',\n        'format': 'lpcm',\n        'sampleRateHertz': 48000,\n    }\n \n    with requests.post(url, headers=headers, data=data, stream=True) as resp:\n        if resp.status_code != 200:\n            raise RuntimeError(\"Invalid response received: code: %d, message: %s\" % (resp.status_code, resp.text))\n \n        for chunk in resp.iter_content(chunk_size=None):\n            yield chunk\n            \n            \ndef create_audiofile(text):\n    filename = 'file2_raw.raw'\n    with open(filename, \"wb\") as f:\n        for audio_content in synthesize(text):\n            f.write(audio_content)\n \n    time.sleep(2)\n \n    return filename\n\ndef convert_raw2wav(input_file=\"file2_raw.raw\", output_file=\"file2_raw.wav\"):\n    output_file = \"file2_raw.wav\"\n\n    cmd = f\"ffmpeg -f s16le -ar 48000 -ac 1 -i {input_file} {output_file}\"\n    os.system(cmd + ' -v quiet')\n    os.remove(input_file)\n\n    \ndef generate_speech_yandex(text):\n    create_audiofile(text)\n    convert_raw2wav()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:09:05.063275Z","iopub.execute_input":"2023-03-29T12:09:05.063748Z","iopub.status.idle":"2023-03-29T12:09:05.077556Z","shell.execute_reply.started":"2023-03-29T12:09:05.063699Z","shell.execute_reply":"2023-03-29T12:09:05.075708Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"# Синтез речи Silero","metadata":{}},{"cell_type":"code","source":"def create_silero_model():\n    torch.hub.download_url_to_file('https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml',\n                                   'latest_silero_models.yml',\n                                   progress=False)\n    models = OmegaConf.load('latest_silero_models.yml')\n    device = torch.device('cpu')\n    torch.set_num_threads(4)\n    local_file = 'model.pt'\n\n    if not os.path.isfile(local_file):\n        torch.hub.download_url_to_file('https://models.silero.ai/models/tts/ru/v3_1_ru.pt',\n                                       local_file)  \n\n    model = torch.package.PackageImporter(local_file).load_pickle(\"tts_models\", \"model\")\n    model.to(device)\n    return model\n\n\ndef audio_synthesis_silero(sentence):\n    ssml_sample = f\"\"\"\n              <speak>                 \n<prosody rate=\"x-fast\">{sentence}</prosody>\n              <speak>\n              \"\"\"\n    sample_rate = 48000\n    speaker='xenia'\n    audio = model.save_wav(text=ssml_sample,\n                             speaker=speaker,\n                             sample_rate=sample_rate,\n                             audio_path='file2_raw.wav'\n                      )","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:20.274865Z","iopub.execute_input":"2023-03-29T09:15:20.275333Z","iopub.status.idle":"2023-03-29T09:15:20.285400Z","shell.execute_reply.started":"2023-03-29T09:15:20.275293Z","shell.execute_reply":"2023-03-29T09:15:20.283671Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Генерация голоса и склейка по временным интервалам","metadata":{}},{"cell_type":"code","source":"\n\ndef get_audio_info(audio_file):\n    ffprobe_command = f'ffprobe -v error -select_streams a:0 -show_entries stream=channels,sample_rate,duration -of default=noprint_wrappers=1:nokey=1 \"{audio_file}\"'\n    output = subprocess.check_output(ffprobe_command + ' -v quiet', shell=True, text=True).strip().split(\"\\n\")\n    sample_rate, channels, input_duration = int(output[0]), int(output[1]), float(output[2])\n    return channels, sample_rate, input_duration\n\ndef audio_boost(audio_file, new_audio_file, atempo):\n    ffmpeg_command = f'ffmpeg -i {audio_file} -filter:a \"atempo={atempo}\" {new_audio_file}'\n    os.system(ffmpeg_command + ' -v quiet')\n    \ndef create_silence(duration, channels, sample_rate):\n    silence_file = \"silence.wav\"\n    ffmpeg_silence_command = f'ffmpeg -f lavfi -i anullsrc=channel_layout={channels}:sample_rate={sample_rate} -t {duration} -c:a pcm_s16le \"{silence_file}\"'\n    os.system(ffmpeg_silence_command + ' -v quiet')\n    \ndef add_silence(audio_file=\"file1.wav\"):\n    merge_two_audio(audio_file, \"silence.wav\")\n\ndef create_audio(sentence, duration, silero=False):\n    # TODO добавить если разница слишком большая сделать суммаризацию\n    if silero:\n        audio_synthesis_silero(sentence)\n    else:\n        generate_speech_yandex(sentence)\n    _, _, input_duration = get_audio_info('file2_raw.wav')\n    print((sentence, 'input_duration', input_duration))\n    print((sentence, 'duration', duration))\n    if input_duration <= duration:\n        os.rename('file2_raw.wav', 'file2.wav')\n    else:\n        temp = input_duration/duration\n        audio_boost('file2_raw.wav', 'file2.wav', temp)\n        os.remove('file2_raw.wav')\n    \ndef merge_two_audio(file_path1, file_path2):\n    output_file = 'merged.wav'\n#     os.system(f'cp {file_path1} 1_{random.randint(0,50)}.wav')\n#     os.system(f'cp {file_path2} 2_{random.randint(50,100)}.wav')\n    ffmpeg_concat_command = f'ffmpeg -i \"{file_path1}\" -i \"{file_path2}\" -filter_complex \"[0:a][1:a]concat=n=2:v=0:a=1\" -c:a pcm_s16le \"{output_file}\"'\n    os.system(ffmpeg_concat_command + ' -v quiet')\n    os.remove(file_path1)\n    os.remove(file_path2)\n#     os.rename(file_path1, f'1_{random.randint(0,20)}.wav')\n#     os.rename(file_path2, f'2_{random.randint(20,40)}.wav')\n    os.rename(output_file, file_path1)\n    \n    \ndef stitching_audio(tiflo, channels, sample_rate, audio_end_time):\n    prev_end = 0.001\n    create_silence(prev_end, channels, sample_rate)\n    os.rename(\"silence.wav\", \"file1.wav\")\n    print(subprocess.check_output('ls',text=True))\n    for line in tiflo:\n#         print(line)\n#         print(subprocess.check_output('ls',text=True))\n        sentence, start, end = line\n        \n        create_silence(start - prev_end, channels, sample_rate)\n#         print(subprocess.check_output('ls',text=True))\n        add_silence()\n        print('add_silence')\n        print(subprocess.check_output('ls',text=True))\n        duration = end - start\n        create_audio(sentence, duration)\n#         print('create_audio')\n#         print(subprocess.check_output('ls',text=True))\n        merge_two_audio(\"file1.wav\", \"file2.wav\")\n#         print('merge_two_audio')\n#         print(subprocess.check_output('ls',text=True))\n        _, _, audio_duration = get_audio_info('file1.wav')\n        silence_time = end - audio_duration\n        prev_end = end - silence_time\n        \n    _, _, audio_duration = get_audio_info('file1.wav')\n    if audio_duration < audio_end_time:\n        silence_duration = audio_end_time - audio_duration\n        create_silence(silence_duration, channels, sample_rate)\n        add_silence()\n        os.rename(\"file1.wav\", \"final.wav\")\n    elif audio_duration > audio_end_time:\n        output_file = 'final.wav'\n        ffmpeg_command = f'ffmpeg -i \"file1.wav\" -t {audio_end_time} -c:a pcm_s16le -map 0:a \"{output_file}\"'\n        os.system(ffmpeg_command + ' -v quiet')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:24.571619Z","iopub.execute_input":"2023-03-29T09:15:24.572929Z","iopub.status.idle":"2023-03-29T09:15:24.593879Z","shell.execute_reply.started":"2023-03-29T09:15:24.572863Z","shell.execute_reply":"2023-03-29T09:15:24.592404Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# display(Audio('filnal.wav', rate=sample_rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Преобразование параметров аудио тифлокомментариев\n","metadata":{}},{"cell_type":"code","source":"def change_sample_rate(audio_file, sample_rate):\n    ffmpeg_command = f'ffmpeg -i {audio_file} -ar {sample_rate} changed_rate.wav'\n    os.system(ffmpeg_command + ' -v quiet')\n    os.remove(audio_file)\n    os.rename('changed_rate.wav', audio_file)\n\n\ndef duration_matching(movie_duration, audio_file):\n    channels, sample_rate, audio_duration = get_audio_info(audio_file)\n    if audio_duration < movie_duration:\n        silence_duration = movie_duration - audio_duration\n        create_silence(silence_duration, channels, sample_rate)\n        add_silence(audio_file=audio_file)\n\n    elif audio_duration > movie_duration:\n        output_file = 'changed_duration.wav'\n        ffmpeg_command = f'ffmpeg -i \"{audio_file}\" -t {movie_duration} -c:a pcm_s16le -map 0:a \"{output_file}\"'\n        os.system(ffmpeg_command + ' -v quiet')\n        os.remove(audio_file)\n        os.rename(output_file, audio_file)\ndef mono2stereo(audio_file):\n#     ffmpeg_command = f'ffmpeg -i {audio_file} -filter_complex \"[0:a]pan=stereo|c0=c1[a]\" -map \"[a]\" added_stereo.wav'\n    ffmpeg_command = f'ffmpeg -i {audio_file} -ac 2 added_stereo.wav'\n    os.system(ffmpeg_command + ' -v quiet')\n    os.remove(audio_file)\n    os.rename('added_stereo.wav', audio_file)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:28.665123Z","iopub.execute_input":"2023-03-29T09:15:28.665534Z","iopub.status.idle":"2023-03-29T09:15:28.675797Z","shell.execute_reply.started":"2023-03-29T09:15:28.665500Z","shell.execute_reply":"2023-03-29T09:15:28.674118Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# os.system(f'cp final.wav 1_final.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Склейка с оригинальной дорожкой","metadata":{}},{"cell_type":"code","source":"def audio_overlay(audio1, audio2, output_file, voice_volume=None):\n    if voice_volume:\n        # команда, если хотим чтобы первое аудио было в n(voice_volume) раз громче второго\n        ffmpeg_command = f'ffmpeg -i {audio1} -i {audio2} -filter_complex amix=inputs=2:duration=longest:weights={voice_volume} {output_file}'\n    else:\n        # наложение с одинаковой громкостью\n        ffmpeg_command = f'ffmpeg -i {audio1} -i {audio2} -filter_complex amix=inputs=2:duration=longest {output_file}'\n    os.system(ffmpeg_command + ' -v quiet')\n    \n\n# os.remove(tiflo_path)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:28:16.082102Z","iopub.execute_input":"2023-03-29T13:28:16.082689Z","iopub.status.idle":"2023-03-29T13:28:16.092026Z","shell.execute_reply.started":"2023-03-29T13:28:16.082638Z","shell.execute_reply":"2023-03-29T13:28:16.090646Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# display(Audio('audio_with_synttiflo.wav', rate=44100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Склейка с видео","metadata":{}},{"cell_type":"code","source":"def add_audio(movie_path, new_audio, replace=True):\n    output_file = 'output_video.mov'\n    if replace:\n        ffmpeg_command = f'ffmpeg -i {movie_path} -i {new_audio} -c:v copy -c:a aac -map 0:v:0 -map 1:a:0 -shortest {output_file}'\n    else:\n        # добавить вторую аудиодорожку в видеофайл без удаления существующей\n        ffmpeg_command = f'ffmpeg -i {movie_path} -i {new_audio} -c:v copy -c:a copy -map 0 -map 1:a:0 {output_file}'\n    os.system(ffmpeg_command + ' -v quiet')","metadata":{"execution":{"iopub.status.busy":"2023-03-29T09:15:42.942039Z","iopub.execute_input":"2023-03-29T09:15:42.942734Z","iopub.status.idle":"2023-03-29T09:15:42.949757Z","shell.execute_reply.started":"2023-03-29T09:15:42.942694Z","shell.execute_reply":"2023-03-29T09:15:42.947914Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"code","source":"\n\n# Извлечение аудиодорожек из фильма\nextract_audio_tracks(movie_path)\n\n## Приведение в нужный формат\nconvert_mp3_to_wav(en_tiflo_orig, en_tiflo)\nchannels_changer()\n\n## Синхронизация аудиодорожек\nru_origin = Path('ru_without.wav')\naudio_en_tiflo = Path('en_tiflo.wav')\naudio_en_without = Path('en_without.wav')\n\nsynchronize_and_process_audio(ru_origin, audio_en_tiflo)\nsynchronize_and_process_audio(ru_origin, audio_en_without)\n\nif whisper_flag:\n#     whisperx_recognize()\n    ## Обработка .ass файлов\n    file_name = '/kaggle/input/audiodescription-nemo/en_tiflo_processed.wav.ass'\n    ass_data = parse_ass(file_name)\n    file_name2 = '/kaggle/input/audiodescription-nemo/en_without_processed.wav.ass'\n    ass_data2 = parse_ass(file_name2)\n    ### Найти все реплики, которые есть во вторых, но нет в первых субтитрах и записать их в список:\n    different_scenes = compare_subtitles(ass_data, ass_data2)\ndifferent_scenes = compare_subtitles(ass_data, ass_data2)\n### Группировка субтитров в предложения\ntiflo_comments = merge_subtitles(different_scenes)\n\n# Перевод GPT API или Яндекс\ngpt_text = '\\n'.join([comment[0] for comment in tiflo_comments])\ntranslated_text = gpt_translate(movie_title, tiflo_comments)\n\n# Собираем текст обратно, проставляем тайминги\ntranslated_tiflo = translation2subs(translated_text, tiflo_comments)\n\nif silero:\n    model = create_silero_model()\n\n# Генерация голоса и склейка по временным интервалам\nchannels_origin, sample_rate_origin, audio_end_time = get_audio_info('ru_without.wav')\nsynthesis_channels = 1\nsynthesis_sample_rate = 48000\nstitching_audio(translated_tiflo, synthesis_channels, synthesis_sample_rate, audio_end_time)\n\n## Преобразование параметров аудио тифлокомментариев\nchannels_orig, sample_rate_orig, duration_orig = get_audio_info(ru_without)\ntiflo_path = 'final.wav'\nchannels_tiflo, sample_rate_tiflo, duration_tiflo = get_audio_info(ru_without)\nchange_sample_rate(tiflo_path, sample_rate_orig)\n## сопоставление длительности\nduration_matching(duration_orig, tiflo_path)\n## добавление стерео\nmono2stereo(tiflo_path)\n\n\n## Склейка с оригинальной дорожкой\nnew_audio = 'audio_with_synttiflo.wav'\naudio_overlay(ru_without, tiflo_path, new_audio, voice_volume=1.4)\n\n## Склейка с видео\nadd_audio(movie_path, new_audio)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:28:19.050725Z","iopub.execute_input":"2023-03-29T13:28:19.052384Z","iopub.status.idle":"2023-03-29T13:29:55.854681Z","shell.execute_reply.started":"2023-03-29T13:28:19.052323Z","shell.execute_reply":"2023-03-29T13:29:55.853113Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stderr","text":"ffmpeg: /opt/conda/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\nffmpeg: /opt/conda/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\nffmpeg: /opt/conda/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\nffmpeg: /opt/conda/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n","output_type":"stream"}]}]}